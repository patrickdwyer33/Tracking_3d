{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66a62913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gc\n",
    "import cv2\n",
    "from torch.utils.data import Dataset\n",
    "import multiprocessing\n",
    "num_cpu_cores = multiprocessing.cpu_count()\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1acd5f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dir_list(input_list):\n",
    "    input_list.sort()\n",
    "    if input_list[0] == \".DS_Store\":\n",
    "        del input_list[0]\n",
    "    if input_list[-1] == \".DS_Store\":\n",
    "        del input_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17b153fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "body_parts = [\n",
    "    \"nose\",\n",
    "    \"left_ear\",\n",
    "    \"right_ear\",\n",
    "    \"top_neck\",\n",
    "    \"left_hip\",\n",
    "    \"right_hip\",\n",
    "    \"tail_base\",\n",
    "    \"tail_end\"\n",
    "]\n",
    "\n",
    "num_body_parts = len(body_parts)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b6e4290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17391290\n",
      "[-0.01869511 -1.96530515 -2.39112972]\n",
      "[ -0.76312399 -31.06215637   1.59425942]\n",
      "17391304\n",
      "[-0.00464728 -0.00911137 -0.00177328]\n",
      "[ 0.28880887 -0.15001639  0.03420693]\n",
      "19412282\n",
      "[-1.52171554  0.9148925   0.84763451]\n",
      "[ -0.12309495 -30.29852551   7.97679411]\n",
      "21340171\n",
      "[-1.36539405 -0.85024314 -0.74972409]\n",
      "[  3.99879834 -30.4969083    5.64596679]\n"
     ]
    }
   ],
   "source": [
    "class Camera:\n",
    "    def __init__(self, name, K, D, T, fisheye):\n",
    "        self.name = name\n",
    "        self.K = K\n",
    "        self.D = D\n",
    "        self.set_T(T)\n",
    "        self.fisheye = fisheye\n",
    "        print(name)\n",
    "        print(self.rvec)\n",
    "        print(self.tvec)\n",
    "        \n",
    "    def set_T(self, new_T):\n",
    "        self.T = new_T\n",
    "        self.update_R_mx()\n",
    "        self.update_R_vec()\n",
    "        self.update_T_vec()\n",
    "        self.update_cam_pos()\n",
    "    \n",
    "    def update_R_mx(self):\n",
    "        self.R = self.T[:3,:3]\n",
    "    \n",
    "    def update_R_vec(self):\n",
    "        self.rvec = cv2.Rodrigues(self.R)[0].flatten()\n",
    "        \n",
    "    def update_T_vec(self):\n",
    "        self.tvec = self.T[:3,3:4].flatten()\n",
    "        \n",
    "    def update_cam_pos(self):\n",
    "        left = -1 * self.R.T\n",
    "        self.cam_pos = left @ self.tvec\n",
    "        \n",
    "N_cams = 4\n",
    "\n",
    "name1 = \"17391290\"\n",
    "name2 = \"17391304\" # top cam\n",
    "name3 = \"19412282\"\n",
    "name4 = \"21340171\"\n",
    "names = [name1,name2,name3,name4]\n",
    "\n",
    "K1 = np.array([[365.99998268853614, 0.0, 653.5836307711236],[0.0, 362.54269203501826, 524.2898251351193],[ 0.0, 0.0, 1.0]], dtype=np.float64)\n",
    "K2 = np.array([[1681.4567542003983, 0.0, 639.5656045632381],[0.0, 1684.09437531831, 511.39077900281455],[0.0, 0.0, 1.0]], dtype=np.float64)\n",
    "K3 = np.array([[365.99998268853614, 0.0, 653.5836307711236],[0.0, 362.54269203501826, 524.2898251351193],[0.0, 0.0, 1.0]], dtype=np.float64)\n",
    "K4 = np.array([[365.99998268853614, 0.0, 653.5836307711236],[0.0, 362.54269203501826, 524.2898251351193],[0.0, 0.0, 1.0]], dtype=np.float64)\n",
    "Ks = [K1,K2,K3,K4]\n",
    "\n",
    "D1 = np.array([0.06606845837702091, -0.06201870735392536, 0.05175078971151844, -0.013604518309950266], dtype=np.float64)\n",
    "D2 = np.array([-0.09329021100102836, -0.015373120930106288, 0.01682495507715212, -0.0272950838970061, 2.051106767874866, 0.09422046169167524, -0.004912591698594109, 1.438207444196221, 0.026447670217746067, 0.005462499898000899, -0.015115712037627299, -0.012697358649902119, -0.029058318376981435, -0.06767148991259063],dtype=np.float64)\n",
    "D3 = np.array([0.06606845837702091, -0.06201870735392536, 0.05175078971151844, -0.013604518309950266], dtype=np.float64)\n",
    "D4 = np.array([0.06606845837702091, -0.06201870735392536, 0.05175078971151844, -0.013604518309950266], dtype=np.float64)\n",
    "Ds = [D1,D2,D3,D4]\n",
    "\n",
    "BigT1 = np.array([[ -0.9988512200551544, 0.043491384384982415, -0.02011814302084402, -0.763123986941524], [ -0.028159114981956473, -0.1930294179994544, 0.9807888192828389, -31.06215637460763], [ 0.038772470101379686, 0.9802286178596653, 0.1940323485689126, 1.594259421824204], [ 0.0, 0.0, 0.0, 1.0]],dtype=np.float64)\n",
    "BigT2 = np.array([[ 0.9999569195887527, 0.001794420968667116, -0.009107086249737644, 0.2888088672435886], [ -0.001752078231171209, 0.9999876292272218, 0.004655278121889946, -0.15001638716214752], [ 0.009115327116719876, -0.004639121243026419, 0.9999476933148291, 0.034206925537038246], [ 0.0, 0.0, 0.0, 1.0]],dtype=np.float64)\n",
    "BigT3 = np.array([[ 0.44291242408315173, -0.895954991190699, -0.033064155111858284, -0.12309494502546652], [ -0.10124781416453565, -0.08662625383473538, 0.991082626360418, -30.298525509057924], [ -0.8908296496635539, -0.4356151350757163, -0.1290813285230491, 7.976794111722069], [ 0.0, 0.0, 0.0, 1.0]],dtype=np.float64)\n",
    "BigT4 = np.array([[ 0.5093793508741311, 0.8569679205886, -0.07834960105259109, 3.998798335582383], [ 0.02952233249985642, 0.07359067539717384, 0.9968514655546011, -30.49690829849623], [ 0.860035527630584, -0.5100886154156347, 0.012185878570874109, 5.645966786117385], [ 0.0, 0.0, 0.0, 1.0]],dtype=np.float64)\n",
    "BigTs = [BigT1,BigT2,BigT3,BigT4]\n",
    "\n",
    "cams = []\n",
    "\n",
    "for i in range(4):\n",
    "    fisheye = True\n",
    "    if i == 1:\n",
    "        fisheye = False\n",
    "    name = names[i]\n",
    "    K = Ks[i]\n",
    "    D = Ds[i]\n",
    "    BigT = BigTs[i]\n",
    "    cam = Camera(name, K, D, BigT, fisheye)\n",
    "    cams.append(cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cb35077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "class utils:\n",
    "    @classmethod\n",
    "    def check_shape(cls, tensor, shape):\n",
    "        tensor_size = tensor.size()\n",
    "        for i in range(len(shape)):\n",
    "            assert(tensor_size[i] == shape[i])\n",
    "\n",
    "    @classmethod\n",
    "    def check_type(cls, tensor, dtype):\n",
    "        assert(tensor.dtype == dtype)\n",
    "\n",
    "    @classmethod\n",
    "    def check_tensor(cls, tensor, shape=None, dtype=None):\n",
    "        if shape is not None:\n",
    "            cls.check_shape(tensor, shape)\n",
    "        if dtype is not None:\n",
    "            cls.check_type(tensor, dtype)\n",
    "\n",
    "    @classmethod\n",
    "    def get_base_imgs(cls, cam_names, base_img_dir):\n",
    "        base_images = []\n",
    "        for cam_name in cam_names:\n",
    "            base_img_path = base_img_dir+\"camera_\"+cam_name+\"_base_img.png\"\n",
    "            base_img = cv2.imread(base_img_path)\n",
    "            base_img = cv2.cvtColor(base_img, cv2.COLOR_BGR2GRAY)\n",
    "            base_img = base_img.astype(\"float32\",copy=False)\n",
    "            base_img /= np.max(base_img)\n",
    "            base_images.append(base_img)\n",
    "        return base_images\n",
    "\n",
    "    @classmethod\n",
    "    def to_numpy(cls, thing):\n",
    "        if isinstance(thing, np.ndarray):\n",
    "            return thing\n",
    "        elif torch.is_tensor(thing):\n",
    "            return thing.detach().cpu().numpy()\n",
    "        elif isinstance(thing, list):\n",
    "            return np.array(thing)\n",
    "        else:\n",
    "            raise TypeError(\"Please pass a list, tensor, or ndarray.\")\n",
    "\n",
    "class ProjectFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, R_vec, T_vec, K_mx, D_vec, fisheye, epsilon):\n",
    "        out_obj = None\n",
    "        input_clone = torch.clone(input)\n",
    "        input_numpy = utils.to_numpy(input_clone)\n",
    "        num_dims = input_numpy.ndim\n",
    "\n",
    "        if num_dims == 2:\n",
    "            input_numpy = input_numpy.reshape(-1,1,3)\n",
    "        assert input_numpy.ndim == 3\n",
    "\n",
    "        R_vec_numpy = utils.to_numpy(R_vec)\n",
    "        T_vec_numpy = utils.to_numpy(T_vec)\n",
    "        # K_mx should be numpy\n",
    "        # D_vec should be numpy\n",
    "        # fisheye should be bool\n",
    "        # epsilon should be float\n",
    "\n",
    "        def project_helper(fisheye, cv_inputs):\n",
    "            out_obj = None\n",
    "            if fisheye:\n",
    "                out_obj = cv2.fisheye.projectPoints(*cv_inputs)\n",
    "            else:\n",
    "                out_obj = cv2.projectPoints(*cv_inputs)\n",
    "            return out_obj\n",
    "\n",
    "        cv_inputs = [input_numpy, R_vec_numpy, T_vec_numpy, K_mx, D_vec]\n",
    "        out_obj = project_helper(fisheye, cv_inputs)\n",
    "\n",
    "        out = out_obj[0].reshape(-1, 2)\n",
    "        proj_jacobian = out_obj[1][:,:6]\n",
    "\n",
    "        proj_jacobian_tensor = torch.tensor(proj_jacobian, dtype=input.dtype, device=input.device)\n",
    "\n",
    "        grad_input_list = []\n",
    "\n",
    "        def estimate_grad(dim, epsilon, fisheye, cv_inputs):\n",
    "            cv_inputs_minus = deepcopy(cv_inputs)\n",
    "            cv_inputs_plus = deepcopy(cv_inputs)\n",
    "            cv_inputs_minus[0][:,:,dim] -= epsilon\n",
    "            cv_inputs_plus[0][:,:,dim] += epsilon\n",
    "            out_minus = torch.tensor(project_helper(fisheye, cv_inputs_minus)[0], dtype=input.dtype, device=input.device)\n",
    "            out_plus = torch.tensor(project_helper(fisheye, cv_inputs_plus)[0], dtype=input.dtype, device=input.device)\n",
    "            dim_grad = out_plus - out_minus\n",
    "            dim_grad /= (2*epsilon)\n",
    "            dim_grad = dim_grad.reshape(-1, 2, 1)\n",
    "            return dim_grad\n",
    "\n",
    "        for i in range(3):\n",
    "            dim_grad = estimate_grad(i, epsilon, fisheye, cv_inputs)\n",
    "            grad_input_list.append(dim_grad)\n",
    "\n",
    "        input_jacobian = torch.cat(grad_input_list, dim=2)\n",
    "        assert input_jacobian.shape[1] == 2\n",
    "        assert input_jacobian.shape[2] == 3\n",
    "        assert input_jacobian.dim() == 3\n",
    "\n",
    "        ctx.save_for_backward(input_jacobian, proj_jacobian_tensor)\n",
    "        out = torch.tensor(out, requires_grad=True, dtype=input.dtype, device=input.device).reshape(-1,2).squeeze()\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input_jacobian, proj_jacobian_tensor = ctx.saved_tensors\n",
    "        grad_input = grad_R_vec = grad_T_vec = grad_K_mx = grad_D_vec = grad_fisheye = grad_epsilon = None\n",
    "        # grad_input.shape = [-1, 3]\n",
    "        # grad_R_vec.shape = [3]\n",
    "        grad_output_shaped = torch.clone(grad_output).view(-1, 1, 2)\n",
    "        grad_input = grad_output_shaped.matmul(input_jacobian).squeeze()\n",
    "        #grad_output_shaped = grad_output_shaped.view(-1, 1)\n",
    "        #grad_params = proj_jacobian_tensor.t().mm(grad_output_shaped).squeeze()\n",
    "        #grad_R_vec = grad_params[:3].squeeze()\n",
    "        #grad_T_vec = grad_params[3:].squeeze()\n",
    "        return grad_input, grad_R_vec, grad_T_vec, grad_K_mx, grad_D_vec, grad_fisheye, grad_epsilon\n",
    "\n",
    "def project(input, R_vec=torch.torch.randn(3,dtype=torch.double), T_vec=torch.randn(3,dtype=torch.double), K_mx=np.random.rand(3,3), D_vec=np.zeros(4,dtype=np.double), fisheye=False, epsilon=1e-6):\n",
    "    return ProjectFunction.apply(input, R_vec, T_vec, K_mx, D_vec, fisheye, epsilon)\n",
    "\n",
    "from torch.autograd import gradcheck\n",
    "input = (torch.randn(10,3,dtype=torch.double,requires_grad=True))\n",
    "test = gradcheck(project, input, eps=1e-6, atol=1e-4)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1e89120",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 9\n",
    "arena_height = 39\n",
    "# points per inch\n",
    "ppi = 5\n",
    "num_x_points = num_y_points = (r * 2 * ppi) + 1\n",
    "num_z_points = (arena_height * ppi) + 1\n",
    "# following two vals were painstakingly tested/computed but are still noisy\n",
    "center_x = -1.199405162390885\n",
    "center_y = -1.6960642799627483\n",
    "x, y, z = np.linspace(-r,r,num_x_points), np.linspace(-r,r,num_y_points), np.linspace(0,arena_height,num_z_points)\n",
    "inner_radius = 3.0\n",
    "outer_radius = 6.693\n",
    "xxx, yyy, zzz = np.meshgrid(x,y,z)\n",
    "all_points = np.rot90(np.vstack(list(map(np.ravel, [xxx,yyy,zzz]))))\n",
    "all_points_tensor = torch.tensor(all_points.copy(),dtype=torch.double,device=device)\n",
    "all_image_points = []\n",
    "with torch.no_grad():\n",
    "    for i in range(N_cams):\n",
    "        cam = cams[i]\n",
    "        image_points = project(all_points_tensor,\n",
    "                               torch.tensor(cam.rvec,dtype=torch.double,device=device),\n",
    "                               torch.tensor(cam.tvec,dtype=torch.double,device=device),\n",
    "                              K_mx=cam.K,\n",
    "                                D_vec=cam.D,\n",
    "                               fisheye=cam.fisheye\n",
    "                              )\n",
    "        all_image_points.append(image_points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7cd41938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bilinear_interp(image, image_points):\n",
    "    # This is just the first formula from wikipedia for bilinear interpolation expressed in py-torch\n",
    "    image_x = image_points[:, 0:1].squeeze()\n",
    "    image_y = image_points[:, 1:2].squeeze()\n",
    "    lower_bound = torch.zeros_like(image_x)\n",
    "    upper_bound_y = torch.ones_like(image_x)*1023\n",
    "    upper_bound_x = torch.ones_like(image_x)*1279 \n",
    "    x1 = torch.floor(image_x)\n",
    "    y1 = torch.floor(image_y)\n",
    "    x2 = x1 + 1\n",
    "    y2 = y1 + 1\n",
    "    x1 = torch.max(torch.min(x1, upper_bound_x),lower_bound)\n",
    "    y1 = torch.max(torch.min(y1, upper_bound_y),lower_bound)\n",
    "    x2 = torch.max(torch.min(x2, upper_bound_x),lower_bound)\n",
    "    y2 = torch.max(torch.min(y2, upper_bound_y),lower_bound)\n",
    "    left = torch.cat(((x2-image_x).reshape(-1,1),(image_x-x1).reshape(-1,1)), axis=1).reshape(-1,1,2)\n",
    "    y1_idx = y1.to(torch.int).detach().cpu().numpy().tolist()\n",
    "    y2_idx = y2.to(torch.int).detach().cpu().numpy().tolist()\n",
    "    x1_idx = x1.to(torch.int).detach().cpu().numpy().tolist()\n",
    "    x2_idx = x2.to(torch.int).detach().cpu().numpy().tolist()\n",
    "    middle_top_left = image[y1_idx,x1_idx].reshape(-1,1)\n",
    "    middle_top_right = image[y2_idx,x1_idx].reshape(-1,1)\n",
    "    middle_top = torch.cat((middle_top_left,middle_top_right),dim=1).reshape(-1,1,2)\n",
    "    middle_bot_left = image[y1_idx,x2_idx].reshape(-1,1)\n",
    "    middle_bot_right = image[y2_idx,x2_idx].reshape(-1,1)\n",
    "    middle_bot = torch.cat((middle_bot_left,middle_bot_right),dim=1).reshape(-1,1,2)\n",
    "    middle = torch.cat((middle_top,middle_bot),dim=1)\n",
    "    right = torch.cat(((y2-image_y).reshape(-1,1),(image_y-y1).reshape(-1,1)), axis=1).reshape(-1,2,1)\n",
    "    middle_right = torch.matmul(middle, right)\n",
    "    middle_right = middle_right.reshape(-1,2,1)\n",
    "    vals = torch.matmul(left, middle_right).reshape(-1)\n",
    "    return vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6af7d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_converter(point, range_begin, range_end, num_points):\n",
    "    interval_length = (range_end - range_begin) / (num_points - 1)\n",
    "    return int((point - range_begin) / interval_length)\n",
    "\n",
    "x_indices = []\n",
    "y_indices = []\n",
    "z_indices = []\n",
    "for point in all_points:\n",
    "    x_idx = idx_converter(point[0], x[0], x[-1], num_x_points)\n",
    "    y_idx = idx_converter(point[1], y[0], y[-1], num_y_points)\n",
    "    z_idx = idx_converter(point[2], z[0], z[-1], num_z_points)\n",
    "    x_indices.append(x_idx)\n",
    "    y_indices.append(y_idx)\n",
    "    z_indices.append(z_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "12ae340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_input(images):\n",
    "    all_vals = []\n",
    "    for i in range(N_cams):\n",
    "        image_points = all_image_points[i]\n",
    "        image = images[i]\n",
    "        interpolated_vals = bilinear_interp(image,image_points).to(torch.double).to(device)\n",
    "        all_vals.append(interpolated_vals)\n",
    "    vals = torch.zeros(N_cams,len(z),len(y),len(x),dtype=torch.double, device=device)\n",
    "    for i in range(N_cams):\n",
    "        vals[i,z_indices,y_indices,x_indices] = all_vals[i]\n",
    "    return vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "316540ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/patrickdwyer/Documents/Scwartz_Lab/Final_Project/notebooks/Training_Data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m             labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(labels)\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out, labels\n\u001b[0;32m---> 42\u001b[0m Training_Data \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerate_model_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36mTrainingData.__init__\u001b[0;34m(self, transform, target_transform, num_body_parts, image_shape_x, image_shape_y, training_data_dir, num_images_per_trial)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, target_transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, num_body_parts\u001b[38;5;241m=\u001b[39mnum_body_parts, image_shape_x\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1280\u001b[39m, image_shape_y\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, training_data_dir\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mgetcwd(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining_Data\u001b[39m\u001b[38;5;124m\"\u001b[39m), num_images_per_trial\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     trial_dirs \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_data_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     process_dir_list(trial_dirs)\n\u001b[1;32m      5\u001b[0m     trial_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(training_data_dir, x), trial_dirs))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/patrickdwyer/Documents/Scwartz_Lab/Final_Project/notebooks/Training_Data'"
     ]
    }
   ],
   "source": [
    "class TrainingData(Dataset):\n",
    "    def __init__(self, transform=None, target_transform=None, num_body_parts=num_body_parts, image_shape_x=1280, image_shape_y=1024, training_data_dir=os.path.join(os.getcwd(), \"Training_Data\"), num_images_per_trial=4):\n",
    "        trial_dirs = os.listdir(training_data_dir)\n",
    "        process_dir_list(trial_dirs)\n",
    "        trial_paths = list(map(lambda x: os.path.join(training_data_dir, x), trial_dirs))\n",
    "        self.data = trial_paths\n",
    "        self.num_images = num_images_per_trial\n",
    "        self.image_shape_x = image_shape_x\n",
    "        self.image_shape_y = image_shape_y\n",
    "        self.num_body_parts = num_body_parts\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        trial_path = self.data[idx]\n",
    "        trial_data = os.listdir(trial_path)\n",
    "        process_dir_list(trial_data)\n",
    "        images = []\n",
    "        for i in range(0, self.num_images):\n",
    "            image_path = os.path.join(trial_path, trial_data[i])\n",
    "            img = cv2.imread(image_path)\n",
    "            img_bw = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            img_bw = img_bw.astype(\"float64\", copy=False)\n",
    "            img_bw /= np.max(img_bw)\n",
    "            images.append(img_bw)\n",
    "        out_images = np.concatenate(images, axis=0)\n",
    "        out = torch.tensor(out_images,dtype=torch.double).reshape(self.num_images,self.image_shape_y,self.image_shape_x)\n",
    "        if self.transform is not None:\n",
    "            out = self.transform(out)\n",
    "        if len(trial_data) != 5:\n",
    "            print(\"WARNING! Labels for training not found.\")\n",
    "            labels = torch.rand(self.num_images*2,self.num_body_parts,dtype=torch.double)\n",
    "            return out, labels\n",
    "        labels_path = os.path.join(trial_path, \"labels.csv\")\n",
    "        labels_numpy = np.genfromtxt(labels_path, delimiter=',')\n",
    "        labels = torch.tensor(labels_numpy, dtype=torch.double).reshape(self.num_images*2,self.num_body_parts)\n",
    "        if self.target_transform is not None:\n",
    "            labels = self.target_transform(labels)\n",
    "        return out, labels\n",
    "\n",
    "Training_Data = TrainingData(transform=generate_model_input)\n",
    "data_loader = None\n",
    "if not torch.cuda.is_available():\n",
    "    data_loader = DataLoader(\n",
    "        Training_Data,\n",
    "        batch_size = 5,\n",
    "        #num_workers = num_cpu_cores,\n",
    "        shuffle = True\n",
    "    )\n",
    "else:\n",
    "    data_loader = DataLoader(\n",
    "        Training_Data,\n",
    "        batch_size = 5,\n",
    "        #num_workers = num_cpu_cores,\n",
    "        shuffle = True,\n",
    "        pin_memory = True\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "37ec584f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        convs = []\n",
    "        padding = 1\n",
    "        stride = 1\n",
    "        poolstride1 = 3\n",
    "        poolstride2 = 2\n",
    "        dilation = 1\n",
    "        print(num_z_points)\n",
    "        print(num_y_points)\n",
    "        print(num_x_points)\n",
    "        final_depth = num_z_points\n",
    "        final_height = num_y_points\n",
    "        final_width = num_x_points\n",
    "        kernel_size1 = 4\n",
    "        kernel_size2 = 2\n",
    "        kernel_pool1 = 3\n",
    "        kernel_pool2 = 2\n",
    "        for i in range(4,8):\n",
    "            conv1 = nn.Conv3d(i,i,kernel_size1,stride=stride,padding=padding)\n",
    "            final_depth = math.floor(((final_depth + (2*padding) - (dilation * (kernel_size1 - 1)) - 1)/stride)+1)\n",
    "            final_height = math.floor(((final_height + (2*padding) - (dilation * (kernel_size1 - 1)) - 1)/stride)+1)\n",
    "            final_width = math.floor(((final_width + (2*padding) - (dilation * (kernel_size1 - 1)) - 1)/stride)+1)\n",
    "            pool1 = nn.MaxPool3d(kernel_pool1,stride=poolstride1,padding=padding)\n",
    "            final_depth = math.floor(((final_depth + (2*padding) - (dilation * (kernel_pool1 - 1)) - 1)/poolstride1)+1)\n",
    "            final_height = math.floor(((final_height + (2*padding) - (dilation * (kernel_pool1 - 1)) - 1)/poolstride1)+1)\n",
    "            final_width = math.floor(((final_width + (2*padding) - (dilation * (kernel_pool1 - 1)) - 1)/poolstride1)+1)\n",
    "            conv2 = nn.Conv3d(i,i+1,kernel_size2,stride=stride,padding=padding)\n",
    "            final_depth = math.floor(((final_depth + (2*padding) - (dilation * (kernel_size2 - 1)) - 1)/stride)+1)\n",
    "            final_height = math.floor(((final_height + (2*padding) - (dilation * (kernel_size2 - 1)) - 1)/stride)+1)\n",
    "            final_width = math.floor(((final_width + (2*padding) - (dilation * (kernel_size2 - 1)) - 1)/stride)+1)\n",
    "            pool2 = nn.MaxPool3d(kernel_pool2,stride=poolstride2,padding=padding)\n",
    "            final_depth = math.floor(((final_depth + (2*padding) - (dilation * (kernel_pool2 - 1)) - 1)/poolstride2)+1)\n",
    "            final_height = math.floor(((final_height + (2*padding) - (dilation * (kernel_pool2 - 1)) - 1)/poolstride2)+1)\n",
    "            final_width = math.floor(((final_width + (2*padding) - (dilation * (kernel_pool2 - 1)) - 1)/poolstride2)+1)\n",
    "            convs.append(conv1)\n",
    "            convs.append(pool1)\n",
    "            convs.append(conv2)\n",
    "            convs.append(pool2)\n",
    "        self.conv_net = nn.Sequential(*convs)\n",
    "        print(final_depth)\n",
    "        print(final_height)\n",
    "        print(final_width)\n",
    "        after_conv_num = final_depth*final_height*final_width*8\n",
    "        self.after_conv_num = after_conv_num\n",
    "        self.l1 = nn.Linear(after_conv_num, after_conv_num //2)\n",
    "        self.l2 = nn.Linear(after_conv_num // 2, 8 * 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv_net(x).view(-1, self.after_conv_num)\n",
    "        lin_out = self.l2(F.relu(self.l1(conv_out)))\n",
    "        out = lin_out.view(-1,8,3)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ee1a32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196\n",
      "91\n",
      "91\n",
      "2\n",
      "2\n",
      "2\n",
      "Model(\n",
      "  (conv_net): Sequential(\n",
      "    (0): Conv3d(4, 4, kernel_size=(4, 4, 4), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (1): MaxPool3d(kernel_size=3, stride=3, padding=1, dilation=1, ceil_mode=False)\n",
      "    (2): Conv3d(4, 5, kernel_size=(2, 2, 2), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (3): MaxPool3d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (4): Conv3d(5, 5, kernel_size=(4, 4, 4), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (5): MaxPool3d(kernel_size=3, stride=3, padding=1, dilation=1, ceil_mode=False)\n",
      "    (6): Conv3d(5, 6, kernel_size=(2, 2, 2), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (7): MaxPool3d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (8): Conv3d(6, 6, kernel_size=(4, 4, 4), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (9): MaxPool3d(kernel_size=3, stride=3, padding=1, dilation=1, ceil_mode=False)\n",
      "    (10): Conv3d(6, 7, kernel_size=(2, 2, 2), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (11): MaxPool3d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (12): Conv3d(7, 7, kernel_size=(4, 4, 4), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (13): MaxPool3d(kernel_size=3, stride=3, padding=1, dilation=1, ceil_mode=False)\n",
      "    (14): Conv3d(7, 8, kernel_size=(2, 2, 2), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (15): MaxPool3d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (l1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (l2): Linear(in_features=32, out_features=24, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "model = model.to(device)\n",
    "model = model.to(torch.double)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8905e98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx1, idx2, length, weight\n",
    "# idxs according to body_parts\n",
    "# nose - left ear\n",
    "# nose - right ear\n",
    "# left ear - neck\n",
    "# right ear - neck\n",
    "# neck - left hip (these two probably aren't constant enough (so zero weight below))\n",
    "# neck - right hip (these two probably aren't constant enough (so zero weight below))\n",
    "# left hip - tail base\n",
    "# right hip - tail base\n",
    "edges = [\n",
    "    [0,1,1,1],\n",
    "    [0,2,1,1],\n",
    "    [1,3,1,1],\n",
    "    [2,3,1,1],\n",
    "    [3,4,1,0],\n",
    "    [3,5,1,0],\n",
    "    [4,6,1,1],\n",
    "    [5,6,1,1]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2095b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "projectors = []\n",
    "for i in range(N_cams):\n",
    "    cam = cams[i]\n",
    "    projector = lambda x: project(x, R_vec=cam.rvec, T_vec=cam.tvec, K_mx=cam.K, D_vec=cam.D, fisheye=cam.fisheye)\n",
    "    projectors.append(projector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01403fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(pred, target, edges=edges, cams=cams, projectors=projectors):\n",
    "    loss = 0.\n",
    "    for edge_info in edges:\n",
    "        weight = edge_info[3]\n",
    "        length = edge_info[2]\n",
    "        dist = (weight * torch.abs(length - torch.square(pred[:,edge_info[0]:edge_info[0]+1,:].squeeze() - pred[:,edge_info[1]:edge_info[1]+1,:].squeeze()))).sum()\n",
    "        loss += dist\n",
    "    for i in range(N_cams):\n",
    "        cam = cams[i]\n",
    "        pred = pred.reshape(-1, 3)\n",
    "        projector = projectors[i]\n",
    "        projected = projector(pred).view(-1)\n",
    "        cur_target = target[:,i*2:(i*2)+2,:].reshape(-1)\n",
    "        dist = torch.square(projected - cur_target).sum()\n",
    "        loss += dist\n",
    "    return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dc60b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for idx, batch in enumerate(dataloader):\n",
    "        batch_in = batch[0]\n",
    "        target = batch[1]\n",
    "        batch_in = batch_in.to(device)\n",
    "        batch_in = batch_in.to(torch.double)\n",
    "        pred = model(batch_in)\n",
    "        loss = loss_fn(pred, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if idx % 100 == 0:\n",
    "            loss_item, current = loss.item(), (idx + 1) * len(batch_in)\n",
    "            print(f\"loss: {loss_item:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381eaa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1\n",
    "for t in range(n_epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(data_loader, model, loss_fn, optimizer)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2339c148",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join(os.getcwd(), \"trained_model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a335cb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2c1332",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
